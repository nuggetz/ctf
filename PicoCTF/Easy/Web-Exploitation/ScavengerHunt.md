#easy #web 

# Scavenger Hunt Walkthrough



This challenge description is vague, but based on experience with similar challenges, the first step is to inspect the website for hidden information.

<img width="730" alt="ScavengerHunt - dash" src="https://github.com/user-attachments/assets/0f0ad8e0-173f-4946-ad0d-ef54e9b09f2a" />


### The challenge

<img width="1015" alt="ScavengerHunt - site" src="https://github.com/user-attachments/assets/3838aa14-c936-4715-8180-ecbccac4f722" />


The site appears simple at first glance, but by inspecting the HTML source code, I find:
1. **A hidden comment** in the HTML file containing the **first part of the flag**.

<img width="810" alt="ScavengerHunt - html first flag" src="https://github.com/user-attachments/assets/b7364875-8fb8-41c2-b4e6-99efc61876f7" />


Since I don’t know how many parts of the flag there are, I continue investigating.
In the **CSS stylesheets**, I find another file that contains the **second part of the flag**.

<img width="1135" alt="ScavengerHunt - html second flag" src="https://github.com/user-attachments/assets/a4b6b3a0-d9ad-4350-84ab-61c57271c6c4" />


In the **JavaScript files** (visible in the browser’s developer tools under the “Sources” tab), there is a curious comment:
“How can I keep Google from indexing my website?”

This prompts us to think about how websites manage search engine indexing.


> [!NOTE]
> **Robots.txt**
> 
> A **robots.txt** file is commonly used to control which parts of a website search engine crawlers can access. While it doesn’t hide pages from manual exploration, it often hints at interesting or restricted files.


To locate it, I navigate to:
challengeURL.com/robots.txt


Here, I find another **hint** and **part of the flag**.

<img width="790" alt="ScavengerHunt - 3flag hint4" src="https://github.com/user-attachments/assets/c92d6934-476d-4448-a42b-b38e314fb66f" />


The robots.txt file includes a note:
“I think it’s an Apache server… can you access the next flag?”
  
This hint suggests checking for hidden Apache server configuration files, such as .htaccess.


> [!NOTE]
> **Why .htaccess?**
> 
> Apache servers often use .htaccess for configuration directives, such as access control, URL rewrites, or directory-level permissions. It can occasionally contain sensitive information if improperly secured.


By visiting:
challengeURL.com/.htaccess


I find another **part of the flag** and a new **hint**:
“I love making websites on my Mac, I can store a lot of information there.”

<img width="457" alt="ScavengerHunt - 4 flag hint5" src="https://github.com/user-attachments/assets/30aa7a5e-06b9-4fd7-bb54-383d2870b264" />


The hint points to **macOS-specific hidden files**, such as .DS_Store.


> [!NOTE]
> **What is .DS_Store?**
> 
> .DS_Store is a file created by macOS to store metadata about the contents of a directory (e.g., icons, display settings). If accessible on a web server, it can sometimes reveal file names or other information within the directory.


Navigating to:
challengeURL.com/DS_Store


I find the **last part of the flag** and successfully complete the scavenger hunt.

<img width="466" alt="ScavengerHunt - last flag" src="https://github.com/user-attachments/assets/555fae16-1aa1-4a6b-9c9d-125daab7e6c4" />


### Conclusion

This challenge highlights the importance of understanding how servers store and expose files and the risks associated with leaving sensitive files accessible.

1. **Inspect Everything:** The browser’s developer tools can reveal hidden comments, files, or hints.
2. **Robots.txt:** While primarily used to manage crawler access, it can inadvertently reveal sensitive directories.
3. **Apache & macOS Files:** Hidden server files like .htaccess or .DS_Store can expose valuable information if not secured properly.

See you in the next challenge!
